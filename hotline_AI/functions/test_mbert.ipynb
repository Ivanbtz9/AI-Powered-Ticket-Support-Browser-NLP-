{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os,re,tqdm\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib,pathlib\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\"data/translated_data\") #take the data translated \n",
    "DATA_PATH_CSV = os.path.join(os.path.dirname(os.getcwd()), \"data/chunks\")\n",
    "\n",
    "try:\n",
    "    data_text = pd.read_csv(os.path.join(DATA_PATH,os.listdir(DATA_PATH)[0]),index_col=0,encoding='utf-8')\n",
    "    for path_chunk in os.listdir(DATA_PATH)[1:]:\n",
    "        data_text = pd.concat( [data_text,pd.read_csv(os.path.join(DATA_PATH,path_chunk),index_col=0,encoding='utf-8')],ignore_index=True)\n",
    "    print('data loaded')\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load and concatenate data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157755"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \"data/chunks\")\n",
    "try:\n",
    "    data_text = pd.read_csv(os.path.join(DATA_PATH, os.listdir(DATA_PATH)[0]), index_col=0, encoding='utf-8')\n",
    "    logging.info(\"Data loaded\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to load and concatenate data: {e}\")\n",
    "    raise\n",
    "\n",
    "#get all questions in a list\n",
    "questions = data_text.question.values.tolist()\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_lower_case=False)\n",
    "# Load the BERT model.\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "Loading MBERT tokenizer.\n",
      "Loading MBERT model.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69320/4281896200.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mbatch_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# take the barycenter (mean) of all embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0msentence_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Concat all embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "   \n",
    "\n",
    "    DATA_PATH = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())),\"data/chunks\") # pathlib.Path(\"/workspaces/Assist_tickets/data/chunks\")\n",
    "\n",
    "    data_text = pd.read_csv(os.path.join(DATA_PATH,os.listdir(DATA_PATH)[0]),index_col=0,encoding='utf-8')\n",
    "    print('data loaded')\n",
    "    \n",
    "    #get all questions in a list\n",
    "    questions = data_text.question.values.tolist() \n",
    "\n",
    "    # Load the BERT tokenizer.\n",
    "    print('Loading MBERT tokenizer.')\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", do_lower_case=False) # all my question are lower, that's why I put do_lower_case=False\n",
    "    print('Loading MBERT model.')\n",
    "    model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    # question is a list of sentence\n",
    "    batch_size = 32  # How can find the max size of the batch size ??\n",
    "\n",
    "    def batchify(data, batch_size):\n",
    "        \"\"\"return list of list len batch size [[\"...\",\"...\",\"...\"],[\"...\",\"...\",\"...\"]] \"\"\"\n",
    "        return [data[i:i + batch_size] for i in range(0, len(data), batch_size)] \n",
    "\n",
    "    \n",
    "    question_batches = batchify(questions[:64], batch_size) #[[\"...\",\"...\",\"...\"],[\"...\",\"...\",\"...\"],[\"...\",\"...\",\"...\"],[\"...\",\"...\",\"...\"]]\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    for batch in question_batches:# add a description : desc=\"Processing batches\" \n",
    "        # here a batch = [\"...\",\"...\",\"...\"]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        with torch.no_grad(): #don't calculate the gradiant to optimize the RAM\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            batch_embeddings = torch.mean(last_hidden_states, dim=1) # take the barycenter (mean) of all embedding\n",
    "            sentence_embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concat all embedding\n",
    "        sentence_embeddings = torch.cat(sentence_embeddings, dim=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
