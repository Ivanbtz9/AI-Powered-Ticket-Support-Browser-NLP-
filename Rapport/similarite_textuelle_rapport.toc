\babel@toc {french}{}\relax 
\contentsline {section}{\numberline {1}Introduction}{2}{section.1}%
\contentsline {section}{\numberline {2}tokenisation et embeddings}{3}{section.2}%
\contentsline {subsection}{\numberline {2.1}Normalisation}{3}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Dictionnaire ou tokenisation}{4}{subsection.2.2}%
\contentsline {subsubsection}{\numberline {2.2.1}Tokenisation basée sur les espaces et la ponctuation}{4}{subsubsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2}Tokenisation en sous-mots}{4}{subsubsection.2.2.2}%
\contentsline {subsection}{\numberline {2.3}Embedding pour les tokens}{7}{subsection.2.3}%
\contentsline {subsubsection}{\numberline {2.3.1}Word2vec}{7}{subsubsection.2.3.1}%
\contentsline {section}{\numberline {3}Un moteur de recherche sans modèle Transformers}{10}{section.3}%
\contentsline {subsection}{\numberline {3.1}La méthode : Term Frequency-Inverse Document Frequency}{10}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}La méthode TF-IDF et le lien avec la théorie de l'information}{12}{subsubsection.3.1.1}%
\contentsline {subsection}{\numberline {3.2}Indice de similarité entre deux documents}{16}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Limites de la méthode TF-IDF}{19}{subsection.3.3}%
\contentsline {section}{\numberline {4}Mécanisme d'attention dans le deep learning}{19}{section.4}%
\contentsline {subsection}{\numberline {4.1}Les modèles Transformers de type encodeur}{20}{subsection.4.1}%
\contentsline {subsubsection}{\numberline {4.1.1}Données d'entrée : embeddings et positional encoding}{20}{subsubsection.4.1.1}%
\contentsline {subsubsection}{\numberline {4.1.2}Architecture d'un modèle de type encodeur}{23}{subsubsection.4.1.2}%
\contentsline {subsubsection}{\numberline {4.1.3}Mécanisme d'attention et multi-têtes}{24}{subsubsection.4.1.3}%
\contentsline {subsection}{\numberline {4.2}Le modèle BERT}{29}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Architecture du modèle BERT}{29}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Introduction de nouveaux tokens}{29}{subsubsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.3}Pré-entraînement du modèle BERT pour la classification}{31}{subsubsection.4.2.3}%
\contentsline {subsubsection}{\numberline {4.2.4}Fine-tuning du modèle BERT pour des tâches de classification}{34}{subsubsection.4.2.4}%
\contentsline {section}{\numberline {5}Un moteur de recherche avec un modèle Transformers}{35}{section.5}%
\contentsline {subsection}{\numberline {5.1}SBERT : un fine-tuning du modèle BERT pour obtenir des embeddings de phrases}{35}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}BERT et les embeddings de phrase}{35}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}SBERT un modèle finetuné de BERT via la méthode des réseaux siamois}{36}{subsubsection.5.1.2}%
\contentsline {subsubsection}{\numberline {5.1.3}Une évaluation difficile des performance}{37}{subsubsection.5.1.3}%
\contentsline {subsection}{\numberline {5.2}Une application suivant les besoins métiers}{38}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}Fonctionnement du moteur de recherche}{38}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}Construction de l'application}{39}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}Les pratiques MLOps }{40}{subsubsection.5.2.3}%
\contentsline {paragraph}{Améliorations :}{40}{subsubsection.5.2.3}%
\contentsline {paragraph}{Déploiement :}{41}{subsubsection.5.2.3}%
\contentsline {paragraph}{Maintenance :}{41}{subsubsection.5.2.3}%
\contentsline {section}{\numberline {6}Conclusion}{42}{section.6}%
\contentsline {paragraph}{Pré-réponses automatiques avec un modèle génératif : }{42}{section.6}%
\contentsline {paragraph}{Utilisation des données sur les pièces machines : }{42}{section.6}%
\contentsline {paragraph}{Amélioration de la normalisation des questions : }{42}{section.6}%
\contentsline {paragraph}{Perspectives futures : }{42}{section.6}%
